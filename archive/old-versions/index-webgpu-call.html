<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Forest Friends Video Call - WebGPU Motion Vectors</title>
  <!-- Firebase SDK -->
  <script src="https://www.gstatic.com/firebasejs/10.7.1/firebase-app-compat.js"></script>
  <script src="https://www.gstatic.com/firebasejs/10.7.1/firebase-database-compat.js"></script>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #87CEEB 0%, #98D98E 50%, #90EE90 100%);
      color: white;
      overflow-x: hidden;
      min-height: 100vh;
      position: relative;
    }

    .container {
      position: relative;
      z-index: 10;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 20px;
    }
    
    .demo-container {
      max-width: 1400px;
      margin: 0 auto;
      padding: 20px;
      display: none;
    }
    
    .avatar-grid {
      display: flex;
      justify-content: center;
      gap: 30px;
      flex-wrap: wrap;
    }

    .avatar-option {
      cursor: pointer;
      transition: all 0.3s ease;
      text-align: center;
    }

    .avatar-option:hover {
      transform: translateY(-5px);
    }

    .avatar-image {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      object-fit: cover;
      border: 4px solid transparent;
      transition: all 0.3s ease;
      box-shadow: 0 5px 20px rgba(0,0,0,0.1);
    }

    .avatar-option:hover .avatar-image {
      box-shadow: 0 10px 30px rgba(0,0,0,0.2);
      border-color: #90EE90;
    }

    .avatar-selected .avatar-image {
      border-color: #4a7c20 !important;
      box-shadow: 0 0 0 3px rgba(74, 124, 32, 0.2);
    }

    h1 {
      text-align: center;
      margin-bottom: 10px;
      font-size: 2em;
    }

    .subtitle {
      text-align: center;
      opacity: 0.9;
      margin-bottom: 30px;
    }

    .controls {
      display: flex;
      gap: 20px;
      justify-content: center;
      margin-bottom: 30px;
      flex-wrap: wrap;
    }

    button, select {
      padding: 10px 20px;
      font-size: 16px;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      background: white;
      color: #667eea;
      transition: all 0.3s;
    }

    button:hover {
      transform: translateY(-2px);
      box-shadow: 0 5px 15px rgba(0,0,0,0.3);
    }

    button.active {
      background: #fbbf24;
      color: #1f2937;
    }

    .demo-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
      gap: 20px;
      margin-bottom: 30px;
    }

    .demo-box {
      background: rgba(255,255,255,0.1);
      border-radius: 10px;
      padding: 15px;
      backdrop-filter: blur(10px);
    }

    .demo-box h3 {
      margin-bottom: 10px;
      font-size: 1.2em;
    }

    canvas {
      width: 100%;
      height: auto;
      border-radius: 5px;
      background: #000;
    }

    video {
      display: none;
    }

    .stats {
      background: rgba(0,0,0,0.5);
      padding: 10px;
      border-radius: 5px;
      margin-top: 10px;
      font-family: monospace;
      font-size: 14px;
    }

    .stat-row {
      display: flex;
      justify-content: space-between;
      margin: 5px 0;
    }

    .stat-label {
      opacity: 0.8;
    }

    .stat-value {
      color: #fbbf24;
      font-weight: bold;
    }

    .error-message {
      background: rgba(255, 0, 0, 0.3);
      padding: 10px;
      border-radius: 5px;
      margin: 20px;
      text-align: center;
      display: none;
    }

    .error-message.show {
      display: block;
    }
  </style>
</head>
<body>
  <div class="container" id="selectionContainer">
    <h1>üå≤ Forest Friends Video Call</h1>
    <p class="subtitle">WebGPU Motion Vectors with Real-time Compression</p>
    <div class="avatar-grid" id="userSelection"></div>
    <div class="status-message" id="statusMessage"></div>
    <div class="error-message" id="errorMessage"></div>
  </div>

  <div class="demo-container" id="demoContainer">
    <h1>üöÄ WebGPU Motion Vectors - Video Call</h1>
    <p class="subtitle">Real-time Video Streaming with Hardware-Accelerated Compression</p>

    <div class="controls">
      <button id="exitBtn" style="background: #ff6b6b; color: white;">Exit Call</button>
      <button id="startBtn">Start Camera</button>
      <select id="blockSize">
        <option value="4" selected>4x4 blocks (adaptive)</option>
        <option value="8">8x8 blocks (fixed)</option>
        <option value="16">16x16 blocks (fixed)</option>
      </select>
      <button id="adaptiveBtn" class="active">Adaptive: ON</button>
      <select id="searchRadius">
        <option value="4">Search ¬±4 pixels</option>
        <option value="8" selected>Search ¬±8 pixels</option>
        <option value="16">Search ¬±16 pixels</option>
      </select>
      <select id="threshold">
        <option value="5">Very Sensitive</option>
        <option value="10">Sensitive</option>
        <option value="20" selected>Normal</option>
        <option value="40">Less Sensitive</option>
        <option value="80">Very Insensitive</option>
      </select>
      <button id="showVectorsBtn">Show Vectors: ON</button>
      <button id="showBlocksBtn">Show Blocks: OFF</button>
      <select id="keyframeInterval">
        <option value="0" selected>Keyframes: Never</option>
        <option value="30">Every 1 second</option>
        <option value="60">Every 2 seconds</option>
        <option value="150">Every 5 seconds</option>
        <option value="300">Every 10 seconds</option>
      </select>
      <select id="bandwidthLimit">
        <option value="50">Bandwidth: 50 KB/s</option>
        <option value="100" selected>Bandwidth: 100 KB/s</option>
        <option value="200">Bandwidth: 200 KB/s</option>
        <option value="500">Bandwidth: 500 KB/s</option>
        <option value="1000">Bandwidth: 1 MB/s</option>
      </select>
      <select id="resolutionSelect">
        <option value="320x240" selected>Resolution: 320x240</option>
        <option value="480x360">Resolution: 480x360</option>
        <option value="640x480">Resolution: 640x480</option>
        <option value="800x600">Resolution: 800x600</option>
        <option value="960x720">Resolution: 960x720</option>
      </select>
    </div>

    <div class="demo-grid">
      <!-- Original Video -->
      <div class="demo-box">
        <h3>üìπ Current Frame</h3>
        <canvas id="currentCanvas" width="320" height="240"></canvas>
        <div class="stats">
          <div class="stat-row">
            <span class="stat-label">Resolution:</span>
            <span class="stat-value" id="resolution">320x240</span>
          </div>
          <div class="stat-row">
            <span class="stat-label">FPS:</span>
            <span class="stat-value" id="fps">0</span>
          </div>
        </div>
      </div>

      <!-- Motion Vectors -->
      <div class="demo-box">
        <h3>‚û°Ô∏è Motion Vectors</h3>
        <canvas id="motionCanvas" width="320" height="240"></canvas>
        <div class="stats">
          <div class="stat-row">
            <span class="stat-label">Blocks with Motion:</span>
            <span class="stat-value" id="motionBlocks">0</span>
          </div>
          <div class="stat-row">
            <span class="stat-label">Avg Motion:</span>
            <span class="stat-value" id="avgMotion">0.0 px</span>
          </div>
          <div class="stat-row">
            <span class="stat-label">Max Motion:</span>
            <span class="stat-value" id="maxMotion">0.0 px</span>
          </div>
        </div>
      </div>

      <!-- Reconstructed with Motion Compensation -->
      <div class="demo-box">
        <h3>üéØ Motion-Compensated Output</h3>
        <canvas id="outputCanvas" width="320" height="240"></canvas>
        <div class="stats">
          <div class="stat-row">
            <span class="stat-label">Events/sec:</span>
            <span class="stat-value" id="eventsPerSec">0</span>
          </div>
          <div class="stat-row">
            <span class="stat-label">Data Size:</span>
            <span class="stat-value" id="dataSize">0 KB</span>
          </div>
          <div class="stat-row">
            <span class="stat-label">Compression:</span>
            <span class="stat-value" id="compression">0x</span>
          </div>
        </div>
      </div>
    </div>
  </div>
  </div>

  <video id="video" playsinline></video>

  <script>
    // Firebase configuration (same as index.html)
    const firebaseConfig = {
      apiKey: "AIzaSyCYZGxmEuqIuDzpmqCBfSkqY7r606xFnvM",
      authDomain: "videocall-signalling.firebaseapp.com",
      databaseURL: "https://videocall-signalling-default-rtdb.firebaseio.com",
      projectId: "videocall-signalling",
      storageBucket: "videocall-signalling.firebasestorage.app",
      messagingSenderId: "1016443322452",
      appId: "1:1016443322452:web:bfe5e3695f41721b5aedc1"
    };
    
    // Initialize Firebase
    firebase.initializeApp(firebaseConfig);
    const database = firebase.database();
    
    // User profiles
    const users = [
      { name: "Miguel", avatar: "avatar-0.webp", emoji: "ü¶î" },
      { name: "Anastasiia", avatar: "avatar-6.webp", emoji: "üêøÔ∏è" },
      { name: "Mariana", avatar: "avatar-5.webp", emoji: "ü¶ú" }
    ];
    
    // Peer management
    let myPeerId = null;
    let myUuid = null; // For compatibility with audio functions
    let peers = new Map();
    let isStreaming = false;
    let localStream = null;
    let audioRecorder = null;
    
    // Audio elements for remote peers
    const audioElements = new Map();
    const audioQueues = new Map();
    
    // Generate unique peer ID
    function generatePeerId() {
      return Math.random().toString(36).substring(2, 15);
    }
    
    // Global state
    let video, isRunning = false;
    let device, context;
    let blockSize = 4; // Start with small blocks for adaptive mode
    let searchRadius = 8;
    let threshold = 20; // Higher default threshold to ignore noise
    let showVectors = true;
    let showBlocks = false; // Default to not showing green squares
    let keyframeInterval = 0; // Default to never (pure motion compensation)
    let adaptiveMode = true; // Default to adaptive quadtree merging
    let videoWidth = 320; // Current resolution width
    let videoHeight = 240; // Current resolution height
    let frameCount = 0;
    let lastTime = performance.now();
    let previousFrame = null;
    let motionVectors = [];
    let eventCount = 0;
    let lastEventTime = performance.now();
    let keyFrame = null; // Store the initial keyframe
    let framesSinceKeyframe = 0;
    let accumulatedFrame = null; // Store the accumulated reconstruction
    let senderReferenceFrame = null; // What the sender thinks receiver has
    let correctionCanvas = null; // Canvas for computing corrections
    let correctionCtx = null;
    
    // Priority system variables
    let maxBandwidthKBps = 100; // Default 100 KB/s (0.1 MB/s) bandwidth limit
    let eventBudget = Math.floor((100 * 1024) / (30 * 100)); // ~34 events at 100KB/s
    
    // Track quad positions for motion vector detection
    let previousQuadMap = new Map(); // Key: "x,y,size" -> quad data

    // Canvas contexts
    const currentCanvas = document.getElementById('currentCanvas');
    const motionCanvas = document.getElementById('motionCanvas');
    const outputCanvas = document.getElementById('outputCanvas');
    const currentCtx = currentCanvas.getContext('2d');
    const motionCtx = motionCanvas.getContext('2d');
    const outputCtx = outputCanvas.getContext('2d');

    // WebGPU variables
    let pipeline = null;
    let bindGroupLayout = null;
    let currentTexture = null;
    let previousTexture = null;
    let motionBuffer = null;

    // Initialize correction canvas for feedback loop
    correctionCanvas = document.createElement('canvas');
    correctionCanvas.width = videoWidth;
    correctionCanvas.height = videoHeight;
    correctionCtx = correctionCanvas.getContext('2d', {
      alpha: false,
      desynchronized: true
    });
    
    // Initialize character selection on page load
    window.addEventListener('load', () => {
      showUserSelection(users);
      updateStatus('Select your avatar to join');
    });
    
    function updateStatus(message) {
      const statusMsg = document.getElementById('statusMessage');
      if (statusMsg) statusMsg.textContent = message;
    }
    
    function showUserSelection(allUsers) {
      const userSelectionDiv = document.getElementById('userSelection');
      userSelectionDiv.innerHTML = '';

      allUsers.forEach(u => {
        const wrapper = document.createElement('div');
        wrapper.className = 'avatar-option';

        const img = document.createElement('img');
        img.src = u.avatar;
        img.alt = u.name;
        img.className = 'avatar-image';

        wrapper.onclick = () => {
          document.querySelectorAll('.avatar-option').forEach(el => 
            el.classList.remove('avatar-selected')
          );
          wrapper.classList.add('avatar-selected');
          
          myUuid = u.name;
          myPeerId = u.name; // Use the same for both
          document.getElementById('errorMessage').textContent = '';
          updateStatus('Joining...');
          
          setTimeout(() => joinCall(), 100);
        };

        wrapper.appendChild(img);
        userSelectionDiv.appendChild(wrapper);
      });
    }
    
    async function joinCall() {
      if (!myUuid) {
        document.getElementById('errorMessage').textContent = 'Please select an avatar!';
        return;
      }

      try {
        updateStatus('Requesting camera and microphone...');
        
        // Hide selection screen and show demo
        document.getElementById('selectionContainer').style.display = 'none';
        document.getElementById('demoContainer').style.display = 'block';
        
        // Start camera with WebGPU motion detection
        await startCamera();
        
      } catch (err) {
        console.error('Error:', err);
        document.getElementById('errorMessage').textContent = 'Error: ' + err.message;
        updateStatus('Failed to connect');
      }
    }
    
    // Initialize controls
    document.getElementById('startBtn').onclick = startCamera;
    document.getElementById('exitBtn').onclick = () => {
      stopCamera();
      document.getElementById('demoContainer').style.display = 'none';
      document.getElementById('selectionContainer').style.display = 'flex';
      
      // Clear any remote canvases
      document.querySelectorAll('[id^="remote-canvas-"]').forEach(canvas => {
        canvas.closest('.demo-box').remove();
      });
      
      // Reset selection
      myPeerId = null;
      myUuid = null;
      updateStatus('Select your avatar to join');
    };
    document.getElementById('blockSize').onchange = (e) => {
      blockSize = parseInt(e.target.value);
      initWebGPU(); // Reinitialize with new block size
    };
    document.getElementById('searchRadius').onchange = (e) => {
      searchRadius = parseInt(e.target.value);
    };
    document.getElementById('threshold').onchange = (e) => {
      threshold = parseInt(e.target.value);
    };
    document.getElementById('showVectorsBtn').onclick = () => {
      showVectors = !showVectors;
      document.getElementById('showVectorsBtn').textContent = `Show Vectors: ${showVectors ? 'ON' : 'OFF'}`;
      document.getElementById('showVectorsBtn').classList.toggle('active', showVectors);
    };
    
    document.getElementById('showBlocksBtn').onclick = () => {
      showBlocks = !showBlocks;
      document.getElementById('showBlocksBtn').textContent = `Show Blocks: ${showBlocks ? 'ON' : 'OFF'}`;
      document.getElementById('showBlocksBtn').classList.toggle('active', showBlocks);
    };
    
    document.getElementById('keyframeInterval').onchange = (e) => {
      keyframeInterval = parseInt(e.target.value);
      framesSinceKeyframe = 0; // Reset counter when interval changes
    };
    
    document.getElementById('adaptiveBtn').onclick = () => {
      adaptiveMode = !adaptiveMode;
      document.getElementById('adaptiveBtn').textContent = `Adaptive: ${adaptiveMode ? 'ON' : 'OFF'}`;
      document.getElementById('adaptiveBtn').classList.toggle('active', adaptiveMode);
      // Force to 4x4 blocks when adaptive mode is on
      if (adaptiveMode && blockSize > 4) {
        blockSize = 4;
        document.getElementById('blockSize').value = '4';
        initWebGPU(); // Reinitialize with new block size
      }
    };
    
    document.getElementById('bandwidthLimit').onchange = (e) => {
      maxBandwidthKBps = parseInt(e.target.value);
      // Calculate event budget based on bandwidth
      // Assuming ~100 bytes per event on average
      eventBudget = Math.floor((maxBandwidthKBps * 1024) / (30 * 100)); // Per frame at 30fps
    };
    
    document.getElementById('resolutionSelect').onchange = async (e) => {
      const [width, height] = e.target.value.split('x').map(Number);
      videoWidth = width;
      videoHeight = height;
      
      // Update canvas sizes
      const canvases = ['currentCanvas', 'motionCanvas', 'outputCanvas'];
      canvases.forEach(id => {
        const canvas = document.getElementById(id);
        canvas.width = width;
        canvas.height = height;
      });
      
      // Update correction canvas size
      if (correctionCanvas) {
        correctionCanvas.width = width;
        correctionCanvas.height = height;
      }
      
      // Update resolution display
      document.getElementById('resolution').textContent = `${width}x${height}`;
      
      // Reinitialize WebGPU with new resolution
      if (device) {
        await initWebGPU();
      }
      
      // Reset accumulated frame since resolution changed
      accumulatedFrame = null;
      senderReferenceFrame = null;
    };

    async function initWebGPU() {
      try {
        // Check for WebGPU support
        if (!navigator.gpu) {
          throw new Error('WebGPU not supported on this browser.');
        }

        // Request adapter and device
        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
          throw new Error('No appropriate GPUAdapter found.');
        }

        device = await adapter.requestDevice();

        // Create shader module
        const shaderModule = device.createShaderModule({
          label: 'Motion Vector Shader',
          code: `
            struct Params {
              blockSize: u32,
              searchRadius: u32,
              threshold: f32,
              width: u32,
              height: u32,
            }

            @group(0) @binding(0) var currentFrame: texture_2d<f32>;
            @group(0) @binding(1) var previousFrame: texture_2d<f32>;
            @group(0) @binding(2) var<uniform> params: Params;
            @group(0) @binding(3) var<storage, read_write> motionVectors: array<vec4<f32>>;
            @group(0) @binding(4) var linearSampler: sampler;

            // Compute block variance and edge strength for better texture classification
            fn computeBlockVariance(blockX: u32, blockY: u32) -> f32 {
              var mean = vec3<f32>(0.0);
              var meanSquared = vec3<f32>(0.0);
              let blockSizeF = f32(params.blockSize * params.blockSize);
              var edgeStrength = 0.0;
              
              // Single pass for mean and variance
              for (var y: u32 = 0u; y < params.blockSize; y = y + 1u) {
                for (var x: u32 = 0u; x < params.blockSize; x = x + 1u) {
                  let pos = vec2<f32>(f32(blockX + x), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height));
                  let pixel = textureSampleLevel(currentFrame, linearSampler, pos, 0.0);
                  mean = mean + pixel.rgb;
                  meanSquared = meanSquared + pixel.rgb * pixel.rgb;
                  
                  // Compute local gradients for edge detection
                  if (x > 0u && x < params.blockSize - 1u && y > 0u && y < params.blockSize - 1u) {
                    let leftPos = vec2<f32>(f32(blockX + x - 1u), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height));
                    let rightPos = vec2<f32>(f32(blockX + x + 1u), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height));
                    let topPos = vec2<f32>(f32(blockX + x), f32(blockY + y - 1u)) / vec2<f32>(f32(params.width), f32(params.height));
                    let bottomPos = vec2<f32>(f32(blockX + x), f32(blockY + y + 1u)) / vec2<f32>(f32(params.width), f32(params.height));
                    
                    let left = textureSampleLevel(currentFrame, linearSampler, leftPos, 0.0).rgb;
                    let right = textureSampleLevel(currentFrame, linearSampler, rightPos, 0.0).rgb;
                    let top = textureSampleLevel(currentFrame, linearSampler, topPos, 0.0).rgb;
                    let bottom = textureSampleLevel(currentFrame, linearSampler, bottomPos, 0.0).rgb;
                    
                    let gradX = abs(right.r - left.r) + abs(right.g - left.g) + abs(right.b - left.b);
                    let gradY = abs(bottom.r - top.r) + abs(bottom.g - top.g) + abs(bottom.b - top.b);
                    edgeStrength = edgeStrength + (gradX + gradY) / 6.0;
                  }
                }
              }
              
              mean = mean / blockSizeF;
              meanSquared = meanSquared / blockSizeF;
              let variance = meanSquared - mean * mean;
              let totalVariance = (variance.r + variance.g + variance.b) / 3.0;
              
              // Combine variance with edge strength for better texture detection
              // High edge strength indicates detailed areas (faces, text)
              // Low variance + low edges = uniform area (wall)
              let edgeContribution = edgeStrength / max(f32(params.blockSize * params.blockSize) / 4.0, 1.0);
              return totalVariance + edgeContribution * 0.5;
            }

            // Enhanced SAD with sub-pixel accuracy and gradient weighting
            fn computeEnhancedSAD(blockX: u32, blockY: u32, offsetX: f32, offsetY: f32) -> f32 {
              var sad: f32 = 0.0;
              var gradientSAD: f32 = 0.0;
              var colorHistogramDiff: f32 = 0.0;
              let blockSizeF = f32(params.blockSize);
              
              // Color histogram bins for better color distribution matching
              var currentHist = array<f32, 8>();
              var prevHist = array<f32, 8>();
              
              for (var y: u32 = 0u; y < params.blockSize; y = y + 1u) {
                for (var x: u32 = 0u; x < params.blockSize; x = x + 1u) {
                  let currentPos = vec2<f32>(f32(blockX + x), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height));
                  // Use bilinear interpolation for sub-pixel accuracy
                  let previousPos = vec2<f32>(f32(blockX + x) + offsetX, f32(blockY + y) + offsetY) / vec2<f32>(f32(params.width), f32(params.height));
                  
                  let currentPixel = textureSampleLevel(currentFrame, linearSampler, currentPos, 0.0);
                  let previousPixel = textureSampleLevel(previousFrame, linearSampler, previousPos, 0.0);
                  
                  // Luminance-weighted SAD (human vision is more sensitive to luminance)
                  let currentLum = 0.299 * currentPixel.r + 0.587 * currentPixel.g + 0.114 * currentPixel.b;
                  let prevLum = 0.299 * previousPixel.r + 0.587 * previousPixel.g + 0.114 * previousPixel.b;
                  let lumDiff = abs(currentLum - prevLum);
                  
                  // Color difference with reduced weight
                  let colorDiff = abs(currentPixel.r - previousPixel.r) * 0.3 + 
                                 abs(currentPixel.g - previousPixel.g) * 0.5 + 
                                 abs(currentPixel.b - previousPixel.b) * 0.2;
                  
                  sad = sad + lumDiff * 2.0 + colorDiff;
                  
                  // Update histograms
                  let currentBin = u32(clamp(currentLum * 7.999, 0.0, 7.999));
                  let prevBin = u32(clamp(prevLum * 7.999, 0.0, 7.999));
                  currentHist[currentBin] = currentHist[currentBin] + 1.0;
                  prevHist[prevBin] = prevHist[prevBin] + 1.0;
                  
                  // Compute gradients for edge-aware matching
                  if (x > 0u && x < params.blockSize - 1u && y > 0u && y < params.blockSize - 1u) {
                    // Sobel operators for better gradient estimation
                    let currentLeft = textureSampleLevel(currentFrame, linearSampler, 
                      vec2<f32>(f32(blockX + x - 1u), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let currentRight = textureSampleLevel(currentFrame, linearSampler, 
                      vec2<f32>(f32(blockX + x + 1u), f32(blockY + y)) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let currentTop = textureSampleLevel(currentFrame, linearSampler,
                      vec2<f32>(f32(blockX + x), f32(blockY + y - 1u)) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let currentBottom = textureSampleLevel(currentFrame, linearSampler,
                      vec2<f32>(f32(blockX + x), f32(blockY + y + 1u)) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    
                    let currentGradX = (currentRight.rgb - currentLeft.rgb) * 0.5;
                    let currentGradY = (currentBottom.rgb - currentTop.rgb) * 0.5;
                    
                    // Previous frame gradients with sub-pixel offset
                    let prevLeft = textureSampleLevel(previousFrame, linearSampler,
                      vec2<f32>(f32(blockX + x - 1u) + offsetX, f32(blockY + y) + offsetY) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let prevRight = textureSampleLevel(previousFrame, linearSampler,
                      vec2<f32>(f32(blockX + x + 1u) + offsetX, f32(blockY + y) + offsetY) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let prevTop = textureSampleLevel(previousFrame, linearSampler,
                      vec2<f32>(f32(blockX + x) + offsetX, f32(blockY + y - 1u) + offsetY) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    let prevBottom = textureSampleLevel(previousFrame, linearSampler,
                      vec2<f32>(f32(blockX + x) + offsetX, f32(blockY + y + 1u) + offsetY) / vec2<f32>(f32(params.width), f32(params.height)), 0.0);
                    
                    let prevGradX = (prevRight.rgb - prevLeft.rgb) * 0.5;
                    let prevGradY = (prevBottom.rgb - prevTop.rgb) * 0.5;
                    
                    // Gradient magnitude difference
                    let currentGradMag = length(currentGradX) + length(currentGradY);
                    let prevGradMag = length(prevGradX) + length(prevGradY);
                    let gradMagDiff = abs(currentGradMag - prevGradMag);
                    
                    // Gradient direction difference
                    let gradDirDiff = length(currentGradX - prevGradX) + length(currentGradY - prevGradY);
                    
                    gradientSAD = gradientSAD + (gradMagDiff + gradDirDiff) * 1.5;
                  }
                }
              }
              
              // Compute histogram difference (Chi-square distance)
              for (var i: u32 = 0u; i < 8u; i = i + 1u) {
                let diff = currentHist[i] - prevHist[i];
                let sum = currentHist[i] + prevHist[i];
                if (sum > 0.0) {
                  colorHistogramDiff = colorHistogramDiff + (diff * diff) / sum;
                }
              }
              
              return sad + gradientSAD + colorHistogramDiff * 10.0;
            }

            @compute @workgroup_size(8, 8)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
              let blockX = global_id.x * params.blockSize;
              let blockY = global_id.y * params.blockSize;
              
              if (blockX >= params.width || blockY >= params.height) {
                return;
              }
              
              // Compute block variance to determine texture complexity
              let blockVariance = computeBlockVariance(blockX, blockY);
              
              // Adaptive threshold based on texture complexity
              // Low variance (uniform areas like walls) = higher threshold (less sensitive)
              // High variance (detailed areas like faces) = lower threshold (more sensitive)
              let adaptiveThreshold = mix(
                params.threshold * 2.0,  // High threshold for uniform areas
                params.threshold * 0.3,  // Low threshold for detailed areas
                clamp(blockVariance * 20.0, 0.0, 1.0)  // Normalize variance to 0-1
              );
              
              // Hierarchical block matching with sub-pixel refinement
              var bestSAD: f32 = 999999.0;
              var bestMV = vec2<f32>(0.0, 0.0);
              
              // Adaptive search parameters based on texture complexity
              let textureComplexity = clamp(blockVariance * 10.0, 0.0, 1.0);
              let isLikelyFace = blockVariance > 0.005 && blockVariance < 0.05; // Faces have moderate variance
              
              // Dynamic search radius
              let adaptiveSearchRadius = i32(mix(
                f32(params.searchRadius) * 0.25,  // Very small for uniform areas
                f32(params.searchRadius) * 1.5,   // Larger for complex areas
                textureComplexity
              ));
              
              // Coarse search first (step size 2 for speed)
              let coarseStep = select(1, 2, adaptiveSearchRadius > 4);
              for (var dy: i32 = -adaptiveSearchRadius; dy <= adaptiveSearchRadius; dy = dy + coarseStep) {
                for (var dx: i32 = -adaptiveSearchRadius; dx <= adaptiveSearchRadius; dx = dx + coarseStep) {
                  // Check boundaries
                  if (i32(blockX) + dx < 0 || i32(blockY) + dy < 0 ||
                      i32(blockX) + dx + i32(params.blockSize) > i32(params.width) ||
                      i32(blockY) + dy + i32(params.blockSize) > i32(params.height)) {
                    continue;
                  }
                  
                  let sad = computeEnhancedSAD(blockX, blockY, f32(dx), f32(dy));
                  
                  // Smart biasing based on texture analysis
                  var bias: f32 = 0.0;
                  
                  // Strong bias against motion in uniform areas
                  if (textureComplexity < 0.1) {
                    bias = 30.0 * sqrt(f32(dx * dx + dy * dy));
                  } 
                  // Slight bias for zero motion in moderately textured areas
                  else if (textureComplexity < 0.3) {
                    bias = 5.0 * sqrt(f32(dx * dx + dy * dy));
                  }
                  // No bias for highly textured areas (likely real detail)
                  else {
                    bias = 0.0;
                  }
                  
                  // Extra sensitivity for face-like regions
                  if (isLikelyFace) {
                    bias = bias * 0.3; // Reduce bias to detect subtle movements
                  }
                  
                  let biasedSAD = sad + bias;
                  
                  if (biasedSAD < bestSAD) {
                    bestSAD = biasedSAD;
                    bestMV = vec2<f32>(f32(dx), f32(dy));
                  }
                }
              }
              
              // Fine search around best coarse match (sub-pixel refinement)
              if (coarseStep > 1 && length(bestMV) > 0.1) {
                let centerX = i32(bestMV.x);
                let centerY = i32(bestMV.y);
                
                for (var dy: i32 = -1; dy <= 1; dy = dy + 1) {
                  for (var dx: i32 = -1; dx <= 1; dx = dx + 1) {
                    if (dx == 0 && dy == 0) { continue; }
                    
                    let testX = centerX + dx;
                    let testY = centerY + dy;
                    
                    if (i32(blockX) + testX < 0 || i32(blockY) + testY < 0 ||
                        i32(blockX) + testX + i32(params.blockSize) > i32(params.width) ||
                        i32(blockY) + testY + i32(params.blockSize) > i32(params.height)) {
                      continue;
                    }
                    
                    let sad = computeEnhancedSAD(blockX, blockY, f32(testX), f32(testY));
                    if (sad < bestSAD) {
                      bestSAD = sad;
                      bestMV = vec2<f32>(f32(testX), f32(testY));
                    }
                  }
                }
                
                // Sub-pixel refinement using parabolic fitting
                if (abs(bestMV.x) > 0.5 && abs(bestMV.y) > 0.5) {
                  // Sample SAD at half-pixel positions
                  let sadLeft = computeEnhancedSAD(blockX, blockY, bestMV.x - 0.5, bestMV.y);
                  let sadRight = computeEnhancedSAD(blockX, blockY, bestMV.x + 0.5, bestMV.y);
                  let sadTop = computeEnhancedSAD(blockX, blockY, bestMV.x, bestMV.y - 0.5);
                  let sadBottom = computeEnhancedSAD(blockX, blockY, bestMV.x, bestMV.y + 0.5);
                  
                  // Parabolic interpolation for sub-pixel accuracy
                  if (sadLeft < bestSAD && sadRight < bestSAD) {
                    let denom = 2.0 * (sadLeft + sadRight - 2.0 * bestSAD);
                    if (abs(denom) > 0.001) {
                      bestMV.x = bestMV.x + (sadLeft - sadRight) / denom * 0.5;
                    }
                  }
                  if (sadTop < bestSAD && sadBottom < bestSAD) {
                    let denom = 2.0 * (sadTop + sadBottom - 2.0 * bestSAD);
                    if (abs(denom) > 0.001) {
                      bestMV.y = bestMV.y + (sadTop - sadBottom) / denom * 0.5;
                    }
                  }
                }
              }
              
              // Store motion vector with confidence
              let blockIndex = global_id.y * (params.width / params.blockSize) + global_id.x;
              
              // Calculate motion magnitude and normalize SAD
              let motionMagnitude = length(bestMV);
              let normalizedSAD = bestSAD / f32(params.blockSize * params.blockSize);
              
              // Calculate confidence score based on multiple factors
              let sadConfidence = 1.0 - clamp(normalizedSAD / adaptiveThreshold, 0.0, 1.0);
              let motionConfidence = clamp(motionMagnitude / f32(params.searchRadius), 0.0, 1.0);
              
              // Different confidence thresholds for different texture types
              var shouldStore = false;
              
              // Highly textured areas (faces, detailed objects)
              if (blockVariance > 0.02) {
                // Face/detail areas - only store if significant motion
                shouldStore = normalizedSAD < adaptiveThreshold && motionMagnitude > 1.0;
              }
              // Moderately textured areas
              else if (blockVariance > 0.005) {
                // Standard threshold - need real motion
                shouldStore = normalizedSAD < adaptiveThreshold * 0.7 && motionMagnitude > 2.0;
              }
              // Uniform areas (walls, backgrounds)
              else {
                // Very strict - almost never update uniform areas
                shouldStore = normalizedSAD < adaptiveThreshold * 0.1 && motionMagnitude > 3.0;
              }
              
              // Store motion vector with confidence in w component
              // ONLY store blocks that actually changed or moved
              let confidence = sadConfidence * mix(0.5, 1.0, clamp(blockVariance * 50.0, 0.0, 1.0));
              
              // Only store if there's real motion or significant change
              if (shouldStore && (motionMagnitude > 0.5 || normalizedSAD > adaptiveThreshold * 0.8)) {
                motionVectors[blockIndex] = vec4<f32>(bestMV.x, bestMV.y, motionMagnitude, confidence);
              } else {
                // No update needed for static blocks
                motionVectors[blockIndex] = vec4<f32>(0.0, 0.0, 0.0, 0.0);
              }
            }
          `
        });

        // Create compute pipeline
        pipeline = device.createComputePipeline({
          label: 'Motion Vector Pipeline',
          layout: 'auto',
          compute: {
            module: shaderModule,
            entryPoint: 'main',
          },
        });

        console.log('WebGPU initialized successfully');
      } catch (error) {
        console.error('WebGPU initialization failed:', error);
        showError(error.message);
      }
    }

    function showError(message) {
      const errorDiv = document.getElementById('errorMessage');
      errorDiv.textContent = message;
      errorDiv.classList.add('show');
    }

    async function startCamera() {
      try {
        video = document.getElementById('video');
        
        // Get both video and audio
        localStream = await navigator.mediaDevices.getUserMedia({ 
          video: { 
            width: { ideal: videoWidth }, 
            height: { ideal: videoHeight } 
          },
          audio: true 
        });
        
        video.srcObject = localStream;
        await video.play();

        // Initialize WebGPU
        await initWebGPU();
        
        // Initialize Firebase peer connection
        await initializePeerConnection();
        
        // Start audio streaming
        startAudioStreaming();

        isRunning = true;
        document.getElementById('startBtn').textContent = 'Stop Camera';
        document.getElementById('startBtn').onclick = stopCamera;
        
        requestAnimationFrame(processFrame);
      } catch (err) {
        console.error('Error:', err);
        showError('Camera error: ' + err.message);
      }
    }

    function stopCamera() {
      isRunning = false;
      isStreaming = false;
      
      // Stop audio recording
      if (audioRecorder && audioRecorder.state !== 'inactive') {
        audioRecorder.stop();
      }
      
      // Clean up Firebase
      if (myPeerId) {
        database.ref(`peers/${myPeerId}`).remove();
        database.ref(`streams/${myPeerId}`).remove();
      }
      
      // Stop all tracks
      if (localStream) {
        localStream.getTracks().forEach(track => track.stop());
      }
      
      if (video && video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop());
      }
      
      document.getElementById('startBtn').textContent = 'Start Camera';
      document.getElementById('startBtn').onclick = startCamera;
    }

    // Handle tab visibility changes
    let isTabVisible = true;
    document.addEventListener('visibilitychange', () => {
      isTabVisible = !document.hidden;
      console.log('Tab visibility changed:', isTabVisible ? 'visible' : 'hidden');
      
      // Force a Firebase update when tab becomes visible
      if (isTabVisible && isStreaming) {
        // Force refresh all listeners
        const streamRef = database.ref(`streams/${myPeerId}`);
        streamRef.once('value', (snapshot) => {
          console.log('Forced refresh of streams after tab switch');
        });
      }
    });
    
    async function processFrame() {
      if (!isRunning) return;
      
      // Don't skip processing even if tab is hidden - keep sending
      // Draw current frame
      currentCtx.drawImage(video, 0, 0, videoWidth, videoHeight);
      const currentImageData = currentCtx.getImageData(0, 0, videoWidth, videoHeight);
      
      // Calculate FPS
      frameCount++;
      const now = performance.now();
      if (now - lastTime > 1000) {
        document.getElementById('fps').textContent = Math.round(frameCount * 1000 / (now - lastTime));
        frameCount = 0;
        lastTime = now;
      }

      // Process motion vectors with WebGPU if we have a previous frame
      if (device && pipeline && previousFrame) {
        await computeMotionVectors(currentImageData, previousFrame);
      }

      // Store current frame as previous for next iteration
      previousFrame = currentImageData;

      requestAnimationFrame(processFrame);
    }

    async function computeMotionVectors(currentFrame, previousFrame) {
      try {
        // Create textures from image data
        const textureSize = { width: videoWidth, height: videoHeight };
        
        // Current frame texture
        const currentTexture = device.createTexture({
          size: textureSize,
          format: 'rgba8unorm',
          usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
        });
        device.queue.writeTexture(
          { texture: currentTexture },
          currentFrame.data,
          { bytesPerRow: videoWidth * 4 },
          textureSize
        );

        // Previous frame texture
        const previousTexture = device.createTexture({
          size: textureSize,
          format: 'rgba8unorm',
          usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
        });
        device.queue.writeTexture(
          { texture: previousTexture },
          previousFrame.data,
          { bytesPerRow: videoWidth * 4 },
          textureSize
        );

        // Create uniform buffer for parameters
        const paramsBuffer = device.createBuffer({
          size: 32, // 5 * 4 bytes, padded to 32
          usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
        });
        const paramsData = new ArrayBuffer(32);
        const paramsView = new DataView(paramsData);
        paramsView.setUint32(0, blockSize, true);
        paramsView.setUint32(4, searchRadius, true);
        paramsView.setFloat32(8, threshold, true);
        paramsView.setUint32(12, videoWidth, true);
        paramsView.setUint32(16, videoHeight, true);
        device.queue.writeBuffer(paramsBuffer, 0, paramsData);

        // Create motion vector output buffer
        const blocksX = Math.ceil(videoWidth / blockSize);
        const blocksY = Math.ceil(videoHeight / blockSize);
        const motionBufferSize = blocksX * blocksY * 16; // vec4<f32> per block
        const motionBuffer = device.createBuffer({
          size: motionBufferSize,
          usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
        });

        // Create sampler
        const sampler = device.createSampler({
          magFilter: 'linear',
          minFilter: 'linear',
        });

        // Create bind group
        const bindGroup = device.createBindGroup({
          layout: pipeline.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: currentTexture.createView() },
            { binding: 1, resource: previousTexture.createView() },
            { binding: 2, resource: { buffer: paramsBuffer } },
            { binding: 3, resource: { buffer: motionBuffer } },
            { binding: 4, resource: sampler },
          ],
        });

        // Encode and submit compute pass
        const commandEncoder = device.createCommandEncoder();
        const computePass = commandEncoder.beginComputePass();
        computePass.setPipeline(pipeline);
        computePass.setBindGroup(0, bindGroup);
        computePass.dispatchWorkgroups(
          Math.ceil(blocksX / 8),
          Math.ceil(blocksY / 8)
        );
        computePass.end();

        // Read back results
        const readBuffer = device.createBuffer({
          size: motionBufferSize,
          usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ,
        });
        commandEncoder.copyBufferToBuffer(motionBuffer, 0, readBuffer, 0, motionBufferSize);

        device.queue.submit([commandEncoder.finish()]);

        // Wait for GPU to finish and read results
        await readBuffer.mapAsync(GPUMapMode.READ);
        const motionData = new Float32Array(readBuffer.getMappedRange());
        
        // Process motion vectors
        visualizeMotionVectors(motionData, blocksX, blocksY);
        
        readBuffer.unmap();

        // Cleanup
        currentTexture.destroy();
        previousTexture.destroy();
        paramsBuffer.destroy();
        motionBuffer.destroy();
        readBuffer.destroy();

      } catch (error) {
        console.error('Motion vector computation failed:', error);
      }
    }

    // Blend two image data objects
    function blendImageData(target, source, alpha) {
      for (let i = 0; i < target.data.length; i += 4) {
        target.data[i] = target.data[i] * (1 - alpha) + source.data[i] * alpha;
        target.data[i + 1] = target.data[i + 1] * (1 - alpha) + source.data[i + 1] * alpha;
        target.data[i + 2] = target.data[i + 2] * (1 - alpha) + source.data[i + 2] * alpha;
      }
    }
    
    // Helper to compute block variance for edge detection
    function computeBlockVariance(imageData, x, y, size) {
      let sum = 0, sumSq = 0, count = 0;
      const width = videoWidth;
      
      for (let dy = 0; dy < size && y + dy < videoHeight; dy++) {
        for (let dx = 0; dx < size && x + dx < width; dx++) {
          const idx = ((y + dy) * width + (x + dx)) * 4;
          const gray = (imageData.data[idx] + imageData.data[idx + 1] + imageData.data[idx + 2]) / 3;
          sum += gray;
          sumSq += gray * gray;
          count++;
        }
      }
      
      if (count === 0) return 0;
      const mean = sum / count;
      const variance = (sumSq / count) - (mean * mean);
      return variance / 65536; // Normalize
    }
    
    // Calculate error/delta for each event compared to reference
    function calculateEventError(event, currentImageData, referenceImageData) {
      let totalError = 0;
      let maxError = 0;
      const width = 320;
      
      // Calculate the total pixel error this event would fix
      for (let dy = 0; dy < event.size; dy++) {
        for (let dx = 0; dx < event.size; dx++) {
          const x = event.x + dx;
          const y = event.y + dy;
          if (x >= width || y >= 240) continue;
          
          const idx = (y * width + x) * 4;
          
          // Calculate how different the current pixel is from reference
          const rDiff = Math.abs(currentImageData.data[idx] - referenceImageData.data[idx]);
          const gDiff = Math.abs(currentImageData.data[idx + 1] - referenceImageData.data[idx + 1]);
          const bDiff = Math.abs(currentImageData.data[idx + 2] - referenceImageData.data[idx + 2]);
          
          const pixelError = (rDiff + gDiff + bDiff) / 3;
          totalError += pixelError;
          maxError = Math.max(maxError, pixelError);
        }
      }
      
      // Store the error magnitude for prioritization
      event.errorMagnitude = totalError;
      event.maxPixelError = maxError;
      
      return totalError;
    }
    
    // Prioritize events based on how much error they fix
    function prioritizeEventsByError(events, budget, currentImageData, referenceImageData) {
      // Calculate error for each event
      events.forEach(event => {
        calculateEventError(event, currentImageData, referenceImageData);
        
        // Priority is based on:
        // 1. Total error this event fixes (most important)
        // 2. Whether it's a correction event
        // 3. Block size (smaller blocks for detail areas)
        
        let priority = event.errorMagnitude; // Base priority on error
        
        // Boost correction events
        if (event.isCorrection) {
          priority *= 1.5;
        }
        
        // Slight preference for smaller blocks when error is similar
        priority += (20 - event.size) * 0.1;
        
        event.priority = priority;
      });
      
      // Sort by priority (highest error first)
      events.sort((a, b) => b.priority - a.priority);
      
      // Take only the top events that fit in budget
      return events.slice(0, budget);
    }
    
    // Compute correction events by comparing reference with actual
    function computeCorrectionEvents(currentImageData, referenceImageData, blockSize) {
      const corrections = [];
      const width = videoWidth;
      const height = videoHeight;
      
      // Check each block for differences
      for (let y = 0; y < height; y += blockSize) {
        for (let x = 0; x < width; x += blockSize) {
          let blockDifference = 0;
          let maxDifference = 0;
          let pixelCount = 0;
          
          // Compute block difference
          for (let dy = 0; dy < blockSize && y + dy < height; dy++) {
            for (let dx = 0; dx < blockSize && x + dx < width; dx++) {
              const idx = ((y + dy) * width + (x + dx)) * 4;
              
              const rDiff = Math.abs(currentImageData.data[idx] - referenceImageData.data[idx]);
              const gDiff = Math.abs(currentImageData.data[idx + 1] - referenceImageData.data[idx + 1]);
              const bDiff = Math.abs(currentImageData.data[idx + 2] - referenceImageData.data[idx + 2]);
              
              const pixelDiff = (rDiff + gDiff + bDiff) / 3;
              blockDifference += pixelDiff;
              maxDifference = Math.max(maxDifference, pixelDiff);
              pixelCount++;
            }
          }
          
          // Average difference for the block
          const avgDifference = blockDifference / pixelCount;
          
          // If block has significant difference, send correction
          // Much more sensitive to catch subtle smears
          const centerX = x + blockSize/2;
          const centerY = y + blockSize/2;
          const isLikelyFace = centerX > 100 && centerX < 220 && centerY > 60 && centerY < 180;
          
          // Even more sensitive thresholds to catch skin-on-wall smears
          const threshold = isLikelyFace ? 3 : 5; // Very sensitive
          
          // Also check for edge/contrast changes that might indicate smearing
          let hasEdgeChange = false;
          if (pixelCount > 0) {
            // Check if this block has lost or gained edges (smear indicator)
            const currentVariance = computeBlockVariance(currentImageData, x, y, blockSize);
            const referenceVariance = computeBlockVariance(referenceImageData, x, y, blockSize);
            hasEdgeChange = Math.abs(currentVariance - referenceVariance) > 0.01;
          }
          
          if (avgDifference > threshold || maxDifference > threshold * 3 || hasEdgeChange) {
            corrections.push({
              x: x,
              y: y,
              size: blockSize,
              mvX: 0, // Correction events have no motion
              mvY: 0,
              confidence: avgDifference / 255, // Confidence based on difference
              isCorrection: true
            });
          }
        }
      }
      
      return corrections;
    }
    
    // Detect if a quad has moved from a previous position
    function detectQuadMotion(events, currentImageData) {
      const enhancedEvents = [];
      const currentQuadMap = new Map();
      
      events.forEach(event => {
        // Extract current quad pixels
        const quadPixels = extractQuadPixels(currentImageData, event.x, event.y, event.size);
        const quadHash = hashQuadPixels(quadPixels);
        
        // Store current quad for next frame
        const quadKey = `${event.x},${event.y},${event.size}`;
        currentQuadMap.set(quadKey, { pixels: quadPixels, hash: quadHash });
        
        // Look for this quad in previous frame at different positions
        let foundMotion = false;
        if (previousQuadMap.size > 0) {
          // Search nearby positions for matching quad
          const searchRange = 16; // Search within 16 pixels
          for (let dy = -searchRange; dy <= searchRange && !foundMotion; dy += 4) {
            for (let dx = -searchRange; dx <= searchRange && !foundMotion; dx += 4) {
              const prevX = event.x - dx;
              const prevY = event.y - dy;
              const prevKey = `${prevX},${prevY},${event.size}`;
              
              const prevQuad = previousQuadMap.get(prevKey);
              if (prevQuad && prevQuad.hash === quadHash) {
                // Found matching quad at different position!
                enhancedEvents.push({
                  ...event,
                  isMotionVector: true,
                  sourceX: prevX,
                  sourceY: prevY,
                  deltaX: dx,
                  deltaY: dy
                });
                foundMotion = true;
              }
            }
          }
        }
        
        if (!foundMotion) {
          // No motion detected, send as pixel update
          enhancedEvents.push({
            ...event,
            isMotionVector: false,
            pixels: quadPixels
          });
        }
      });
      
      // Update previous quad map for next frame
      previousQuadMap = currentQuadMap;
      
      return enhancedEvents;
    }
    
    // Extract pixels from a quad
    function extractQuadPixels(imageData, x, y, size) {
      const pixels = [];
      for (let dy = 0; dy < size; dy++) {
        for (let dx = 0; dx < size; dx++) {
          const idx = ((y + dy) * videoWidth + (x + dx)) * 4;
          pixels.push(imageData.data[idx]);     // R
          pixels.push(imageData.data[idx + 1]); // G
          pixels.push(imageData.data[idx + 2]); // B
        }
      }
      return pixels;
    }
    
    // Create a simple hash of quad pixels for comparison
    function hashQuadPixels(pixels) {
      // Simple hash: sample every 4th pixel and combine
      let hash = 0;
      for (let i = 0; i < pixels.length; i += 12) { // Sample every 4th pixel (3 bytes * 4)
        hash = ((hash << 5) - hash) + pixels[i] + pixels[i+1] + pixels[i+2];
        hash = hash & 0xFFFFFF; // Keep it 24-bit
      }
      return hash;
    }
    
    // Adaptive quadtree merging of motion vectors
    function mergeAdaptiveBlocks(motionData, blocksX, blocksY) {
      if (!adaptiveMode) {
        // No merging in fixed mode - return ALL blocks (moving or changed)
        const events = [];
        for (let by = 0; by < blocksY; by++) {
          for (let bx = 0; bx < blocksX; bx++) {
            const idx = (by * blocksX + bx) * 4;
            const mvX = motionData[idx];
            const mvY = motionData[idx + 1];
            const magnitude = motionData[idx + 2];
            const confidence = motionData[idx + 3];
            
            // Only include blocks with significant motion or change
            // Higher threshold to ignore noise
            if (confidence > 0.3 || magnitude > 0.5) {
              events.push({
                x: bx * blockSize,
                y: by * blockSize,
                mvX: mvX,
                mvY: mvY,
                size: blockSize,
                confidence: confidence,
                needsUpdate: true
              });
            }
          }
        }
        return events;
      }
      
      // Adaptive mode - merge similar neighboring blocks
      const merged = [];
      const visited = new Set();
      
      // Try to form larger blocks (16x16, 8x8) from 4x4 base blocks
      for (let by = 0; by < blocksY; by += 4) {
        for (let bx = 0; bx < blocksX; bx += 4) {
          // Try 16x16 first
          if (canMerge16x16(motionData, bx, by, blocksX, blocksY, visited)) {
            const idx = (by * blocksX + bx) * 4;
            merged.push({
              x: bx * blockSize,
              y: by * blockSize,
              mvX: motionData[idx],
              mvY: motionData[idx + 1],
              size: 16,
              confidence: motionData[idx + 3]
            });
            // Mark all sub-blocks as visited
            for (let dy = 0; dy < 4; dy++) {
              for (let dx = 0; dx < 4; dx++) {
                visited.add(`${bx + dx},${by + dy}`);
              }
            }
          }
        }
      }
      
      // Try 8x8 blocks for remaining areas
      for (let by = 0; by < blocksY; by += 2) {
        for (let bx = 0; bx < blocksX; bx += 2) {
          if (!visited.has(`${bx},${by}`) && canMerge8x8(motionData, bx, by, blocksX, blocksY, visited)) {
            const idx = (by * blocksX + bx) * 4;
            merged.push({
              x: bx * blockSize,
              y: by * blockSize,
              mvX: motionData[idx],
              mvY: motionData[idx + 1],
              size: 8,
              confidence: motionData[idx + 3]
            });
            // Mark sub-blocks as visited
            for (let dy = 0; dy < 2; dy++) {
              for (let dx = 0; dx < 2; dx++) {
                visited.add(`${bx + dx},${by + dy}`);
              }
            }
          }
        }
      }
      
      // Add remaining 4x4 blocks that couldn't be merged
      for (let by = 0; by < blocksY; by++) {
        for (let bx = 0; bx < blocksX; bx++) {
          if (!visited.has(`${bx},${by}`)) {
            const idx = (by * blocksX + bx) * 4;
            const confidence = motionData[idx + 3];
            const magnitude = motionData[idx + 2];
            
            if (confidence > 0.3 || magnitude > 0.5) {
              merged.push({
                x: bx * blockSize,
                y: by * blockSize,
                mvX: motionData[idx],
                mvY: motionData[idx + 1],
                size: 4,
                confidence: confidence
              });
            }
          }
        }
      }
      
      return merged;
    }
    
    function canMerge16x16(motionData, bx, by, blocksX, blocksY, visited) {
      if (bx + 3 >= blocksX || by + 3 >= blocksY) return false;
      
      // Check if all 4x4 sub-blocks have similar motion
      const baseIdx = (by * blocksX + bx) * 4;
      const baseMvX = motionData[baseIdx];
      const baseMvY = motionData[baseIdx + 1];
      const baseConfidence = motionData[baseIdx + 3];
      
      // Need minimum confidence to merge
      if (baseConfidence < 0.2) return false;
      
      // Check all 16 sub-blocks
      for (let dy = 0; dy < 4; dy++) {
        for (let dx = 0; dx < 4; dx++) {
          if (visited.has(`${bx + dx},${by + dy}`)) return false;
          
          const idx = ((by + dy) * blocksX + (bx + dx)) * 4;
          const mvX = motionData[idx];
          const mvY = motionData[idx + 1];
          const confidence = motionData[idx + 3];
          
          // Check if motion vectors are similar (within 1 pixel)
          if (Math.abs(mvX - baseMvX) > 1.0 || 
              Math.abs(mvY - baseMvY) > 1.0 ||
              confidence < 0.2) {
            return false;
          }
        }
      }
      return true;
    }
    
    function canMerge8x8(motionData, bx, by, blocksX, blocksY, visited) {
      if (bx + 1 >= blocksX || by + 1 >= blocksY) return false;
      
      const baseIdx = (by * blocksX + bx) * 4;
      const baseMvX = motionData[baseIdx];
      const baseMvY = motionData[baseIdx + 1];
      const baseConfidence = motionData[baseIdx + 3];
      
      if (baseConfidence < 0.25) return false;
      
      // Check all 4 sub-blocks
      for (let dy = 0; dy < 2; dy++) {
        for (let dx = 0; dx < 2; dx++) {
          if (visited.has(`${bx + dx},${by + dy}`)) return false;
          
          const idx = ((by + dy) * blocksX + (bx + dx)) * 4;
          const mvX = motionData[idx];
          const mvY = motionData[idx + 1];
          const confidence = motionData[idx + 3];
          
          if (Math.abs(mvX - baseMvX) > 0.5 || 
              Math.abs(mvY - baseMvY) > 0.5 ||
              confidence < 0.25) {
            return false;
          }
        }
      }
      return true;
    }

    function visualizeMotionVectors(motionData, blocksX, blocksY) {
      // Clear motion canvas
      motionCtx.fillStyle = 'rgba(0, 0, 0, 0.3)';
      motionCtx.fillRect(0, 0, videoWidth, videoHeight);

      // Draw current frame as background
      motionCtx.globalAlpha = 0.3;
      motionCtx.drawImage(currentCanvas, 0, 0);
      motionCtx.globalAlpha = 1.0;

      // Get motion events from GPU
      let events = mergeAdaptiveBlocks(motionData, blocksX, blocksY);
      
      // Detect quad motion BEFORE feedback loop
      const currentImageData = currentCtx.getImageData(0, 0, videoWidth, videoHeight);
      events = detectQuadMotion(events, currentImageData);
      
      // FEEDBACK LOOP: Compare sender reference with actual current frame
      if (senderReferenceFrame) {
        const corrections = computeCorrectionEvents(currentImageData, senderReferenceFrame, blockSize);
        
        // Merge corrections with motion events, avoiding duplicates
        const eventMap = new Map();
        
        // Add motion events first
        events.forEach(event => {
          const key = `${event.x},${event.y}`;
          eventMap.set(key, event);
        });
        
        // Add or update with correction events
        corrections.forEach(correction => {
          const key = `${correction.x},${correction.y}`;
          if (!eventMap.has(key)) {
            // New correction block
            eventMap.set(key, correction);
          } else {
            // Update existing event with correction flag
            const existing = eventMap.get(key);
            existing.needsCorrection = true;
            existing.isCorrection = true;
          }
        });
        
        // Convert back to array
        events = Array.from(eventMap.values());
        
        // Prioritize events by how much error they fix
        events = prioritizeEventsByError(events, eventBudget, currentImageData, senderReferenceFrame);
      } else {
        // No reference frame yet, just limit to budget
        events = events.slice(0, eventBudget);
      }
      
      let motionCount = events.length;
      let totalMotion = 0;
      let maxMotion = 0;
      let blockStats = { 4: 0, 8: 0, 16: 0 };
      let correctionCount = events.filter(e => e.isCorrection).length;
      let motionVectorCount = events.filter(e => e.isMotionVector).length;

      // Draw motion vectors for merged blocks
      events.forEach(event => {
        let magnitude, vectorX, vectorY;
        
        if (event.isMotionVector) {
          // For true motion vectors, use the delta
          magnitude = Math.sqrt(event.deltaX * event.deltaX + event.deltaY * event.deltaY);
          vectorX = event.deltaX;
          vectorY = event.deltaY;
        } else {
          // For regular events, use mvX/mvY from GPU
          magnitude = Math.sqrt(event.mvX * event.mvX + event.mvY * event.mvY);
          vectorX = event.mvX;
          vectorY = event.mvY;
        }
        
        totalMotion += magnitude;
        maxMotion = Math.max(maxMotion, magnitude);
        
        // Count block sizes
        blockStats[event.size] = (blockStats[event.size] || 0) + 1;

        const centerX = event.x + event.size / 2;
        const centerY = event.y + event.size / 2;

        if (showVectors && magnitude > 0.1) {
          // Draw motion vector arrow - blue for true motion vectors, colored by magnitude for others
          if (event.isMotionVector) {
            motionCtx.strokeStyle = 'rgba(100, 150, 255, 0.9)'; // Blue for motion vectors
            motionCtx.lineWidth = 2;
          } else {
            motionCtx.strokeStyle = `hsl(${120 - magnitude * 10}, 100%, 50%)`;
            motionCtx.lineWidth = Math.max(1, magnitude / 4);
          }
          
          motionCtx.beginPath();
          motionCtx.moveTo(centerX, centerY);
          motionCtx.lineTo(centerX + vectorX * 2, centerY + vectorY * 2);
          motionCtx.stroke();

          // Draw arrowhead
          const angle = Math.atan2(vectorY, vectorX);
          motionCtx.fillStyle = motionCtx.strokeStyle;
          motionCtx.beginPath();
          motionCtx.moveTo(centerX + vectorX * 2, centerY + vectorY * 2);
          motionCtx.lineTo(
            centerX + vectorX * 2 - 5 * Math.cos(angle - Math.PI / 6),
            centerY + vectorY * 2 - 5 * Math.sin(angle - Math.PI / 6)
          );
          motionCtx.lineTo(
            centerX + vectorX * 2 - 5 * Math.cos(angle + Math.PI / 6),
            centerY + vectorY * 2 - 5 * Math.sin(angle + Math.PI / 6)
          );
          motionCtx.closePath();
          motionCtx.fill();
        }

        // Draw block outline with color based on size
        const blockColors = {
          4: 'rgba(255, 100, 100, 0.5)',  // Red for small
          8: 'rgba(255, 255, 100, 0.5)',  // Yellow for medium
          16: 'rgba(100, 255, 100, 0.5)'  // Green for large
        };
        motionCtx.strokeStyle = blockColors[event.size] || 'rgba(255, 255, 255, 0.5)';
        motionCtx.lineWidth = 1;
        motionCtx.strokeRect(event.x, event.y, event.size, event.size);
      });

      // Update stats
      const highPriorityBlocks = events.filter(e => e.errorMagnitude > 1000).length;
      const totalError = events.reduce((sum, e) => sum + (e.errorMagnitude || 0), 0);
      const mvInfo = motionVectorCount > 0 ? ` MV:${motionVectorCount}` : '';
      const blockInfo = `${motionCount}/${eventBudget}${mvInfo}`;
      document.getElementById('motionBlocks').textContent = blockInfo;
      document.getElementById('avgMotion').textContent = motionCount > 0 ? 
        (totalMotion / motionCount).toFixed(1) + ' px' : '0.0 px';
      document.getElementById('maxMotion').textContent = maxMotion.toFixed(1) + ' px';

      // Update event stats
      eventCount += events.length;
      const now = performance.now();
      if (now - lastEventTime > 1000) {
        // Keep the events per second display separate
        const eventsText = eventCount.toString();
        document.getElementById('eventsPerSec').textContent = eventsText;
        eventCount = 0;
        lastEventTime = now;
      }

      // Calculate ACTUAL data size that would be transmitted
      let dataSize = 0;
      events.forEach(event => {
        if (event.isMotionVector) {
          // Motion vector: position (2 bytes), delta (2 bytes), size (1 byte) = 5 bytes
          // Plus 1 byte for event type = 6 bytes total
          // Let's round to 8 bytes for alignment
          dataSize += 8;
        } else {
          // Pixel update: metadata + compressed pixels
          dataSize += 5; // Metadata
          
          // Actual pixel data for the block
          const pixelCount = event.size * event.size;
          const rawBlockSize = pixelCount * 3; // RGB bytes
          const compressedBlockSize = Math.ceil(rawBlockSize / 10); // JPEG compression estimate
          dataSize += compressedBlockSize;
        }
      });
      
      // Convert to KB
      const dataSizeKB = dataSize / 1024;
      document.getElementById('dataSize').textContent = dataSizeKB.toFixed(2) + ' KB/frame';
      
      // Raw frame size
      const rawSize = videoWidth * videoHeight * 3; // Uncompressed RGB
      const compressionRatio = dataSize > 0 ? Math.round(rawSize / dataSize) : 999;
      document.getElementById('compression').textContent = compressionRatio + 'x';
      
      // Show bandwidth estimate in the data size field
      const bandwidthMbps = (dataSize * 30 * 8) / (1024 * 1024); // Megabits per second
      document.getElementById('dataSize').textContent = 
        `${dataSizeKB.toFixed(2)} KB (${bandwidthMbps.toFixed(1)} Mbps)`;

      // Apply motion compensation to output
      applyMotionCompensation(events);
      
      // Update sender reference frame (what we think receiver has)
      senderReferenceFrame = outputCtx.getImageData(0, 0, videoWidth, videoHeight);
    }

    function applyMotionCompensation(events) {
      // For true motion compensation without keyframes, we need to:
      // 1. Clear areas where blocks moved FROM (to prevent smearing)
      // 2. Draw blocks at their new positions
      
      framesSinceKeyframe++;
      
      // Initial frame setup (only happens once)
      if (!accumulatedFrame) {
        // First frame needs to be complete
        accumulatedFrame = currentCtx.getImageData(0, 0, videoWidth, videoHeight);
        senderReferenceFrame = currentCtx.getImageData(0, 0, videoWidth, videoHeight); // Initialize sender reference
        outputCtx.putImageData(accumulatedFrame, 0, 0);
        
        outputCtx.fillStyle = 'rgba(255, 255, 0, 0.5)';
        outputCtx.font = 'bold 16px monospace';
        outputCtx.fillText('INITIAL FRAME', 10, 30);
        return;
      }
      
      // Optional keyframe reset (but we want to avoid this)
      if (keyframeInterval > 0 && framesSinceKeyframe >= keyframeInterval) {
        accumulatedFrame = currentCtx.getImageData(0, 0, videoWidth, videoHeight);
        framesSinceKeyframe = 0;
        outputCtx.putImageData(accumulatedFrame, 0, 0);
        
        outputCtx.fillStyle = 'rgba(255, 255, 0, 0.5)';
        outputCtx.font = 'bold 16px monospace';
        outputCtx.fillText('KEYFRAME RESET', 10, 30);
        return;
      }

      // Start with previous accumulated frame
      outputCtx.putImageData(accumulatedFrame, 0, 0);
      
      // Apply motion compensation: update blocks that changed or moved
      events.forEach(event => {
        if (event.isMotionVector) {
          // Motion vector: copy quad from its source position in accumulated frame
          const blockData = outputCtx.createImageData(event.size, event.size);
          
          // Copy from source position (where the quad came from)
          for (let dy = 0; dy < event.size; dy++) {
            for (let dx = 0; dx < event.size; dx++) {
              const srcX = event.sourceX + dx;
              const srcY = event.sourceY + dy;
              const dstIdx = (dy * event.size + dx) * 4;
              const srcIdx = (srcY * videoWidth + srcX) * 4;
              
              if (srcX >= 0 && srcX < videoWidth && srcY >= 0 && srcY < videoHeight) {
                blockData.data[dstIdx] = accumulatedFrame.data[srcIdx];
                blockData.data[dstIdx + 1] = accumulatedFrame.data[srcIdx + 1];
                blockData.data[dstIdx + 2] = accumulatedFrame.data[srcIdx + 2];
                blockData.data[dstIdx + 3] = 255;
              }
            }
          }
          
          // Put at destination position
          outputCtx.putImageData(blockData, event.x, event.y);
        } else {
          // Regular pixel update: get block from current frame
          const blockData = currentCtx.getImageData(event.x, event.y, event.size, event.size);
          outputCtx.putImageData(blockData, event.x, event.y);
        }
      });
      
      // Save the current output as the new accumulated frame
      accumulatedFrame = outputCtx.getImageData(0, 0, videoWidth, videoHeight);
      
      // Send motion events with block data (like original plan!)
      if (isStreaming) {
        // Send a keyframe periodically (every 2 seconds)
        if (frameCount % 60 === 0) {
          // Send full frame as keyframe
          const keyframeData = currentCanvas.toDataURL('image/jpeg', 0.4);
          sendKeyframe(keyframeData.split(',')[1]);
        } else if (events.length > 0) {
          // Send motion events
          // Limit to reasonable number to avoid overwhelming Firebase
          const eventsToSend = events.slice(0, 30); // Reduced limit since we're sending image data
        
        // Convert events to transmittable format WITH block image data
        const transmittableEvents = eventsToSend.map(event => {
          // Get the actual block pixels from current frame
          const blockData = currentCtx.getImageData(event.x, event.y, event.size, event.size);
          
          // Convert block to base64 (small blocks compress well)
          const blockCanvas = document.createElement('canvas');
          blockCanvas.width = event.size;
          blockCanvas.height = event.size;
          const blockCtx = blockCanvas.getContext('2d');
          blockCtx.putImageData(blockData, 0, 0);
          
          // Use high compression for small blocks
          const dataUrl = blockCanvas.toDataURL('image/jpeg', 0.7);
          const base64Data = dataUrl.split(',')[1];
          
          return {
            x: event.x,
            y: event.y,
            s: event.size,
            d: base64Data, // The actual pixel data!
            mv: event.isMotionVector ? 1 : 0,
            vx: Math.round(event.mvX || 0),
            vy: Math.round(event.mvY || 0)
          };
        });
        
          if (transmittableEvents.length > 0) {
            sendMotionEvents(transmittableEvents);
          }
        }
      }
      
      // Show block outlines if enabled
      if (showBlocks) {
        events.forEach(event => {
          const blockColors = {
            4: 'rgba(255, 100, 100, 0.5)',
            8: 'rgba(255, 255, 100, 0.5)',
            16: 'rgba(100, 255, 100, 0.5)'
          };
          
          // Correction blocks shown in cyan
          if (event.isCorrection) {
            outputCtx.strokeStyle = 'rgba(0, 255, 255, 0.7)';
            outputCtx.lineWidth = 2;
          } else {
            outputCtx.strokeStyle = blockColors[event.size] || 'rgba(255, 255, 255, 0.5)';
            outputCtx.lineWidth = 1;
          }
          outputCtx.strokeRect(event.x, event.y, event.size, event.size);
        });
      }
      
      // Show stats
      outputCtx.fillStyle = 'rgba(0, 255, 0, 0.8)';
      outputCtx.font = '12px monospace';
      outputCtx.fillText(`Events: ${events.length}`, 10, 20);
      
      if (keyframeInterval === 0) {
        outputCtx.fillStyle = 'rgba(100, 255, 100, 0.8)';
        outputCtx.fillText('Pure motion compensation (no keyframes)', 10, 35);
        
        // Track coverage to detect areas that need updating
        const coverage = (events.length * 16) / (videoWidth * videoHeight) * 100;
        outputCtx.fillText(`Coverage: ${coverage.toFixed(1)}%`, 10, 50);
      } else {
        const secondsSinceKeyframe = (framesSinceKeyframe / 30).toFixed(1);
        outputCtx.fillStyle = 'rgba(200, 200, 200, 0.8)';
        outputCtx.fillText(`${secondsSinceKeyframe}s since keyframe`, 10, 35);
      }
    }
    
    // ============= Audio Streaming Functions =============
    
    function startAudioStreaming() {
      if (!localStream) return;
      
      const audioTrack = localStream.getAudioTracks()[0];
      if (!audioTrack) {
        console.log('No audio track available');
        return;
      }
      
      try {
        // Use MediaRecorder to create playable audio segments
        const audioStream = new MediaStream([audioTrack]);
        audioRecorder = new MediaRecorder(audioStream, {
          mimeType: 'audio/webm;codecs=opus',
          audioBitsPerSecond: 64000 // Higher bitrate for better quality
        });
        
        let audioSequence = 0;
        let audioChunks = [];
        
        audioRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };
        
        // Every 200ms, send accumulated audio as a playable segment
        setInterval(async () => {
          if (audioChunks.length > 0) {
            // Combine all chunks into one blob
            const audioBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });
            audioChunks = []; // Clear for next segment
            
            // Convert to base64
            const reader = new FileReader();
            reader.onloadend = async () => {
              const dataUrl = reader.result;
              const base64 = dataUrl.split(',')[1];
              
              if (!base64 || base64.length > 100000) {
                console.warn('Audio segment issue:', base64 ? base64.length : 'no data');
                return;
              }
              
              const audioData = {
                s: audioSequence++,
                d: base64,
                t: Date.now()
              };
              
              // Send to all active peers except self
              const activePeers = await getActivePeers();
              for (const peerId of activePeers) {
                if (peerId !== myUuid) {
                  database.ref(`streams/${peerId}/audio_${myUuid}`).set(audioData)
                    .catch(e => console.error(`Failed to send audio to ${peerId}:`, e));
                }
              }
            };
            reader.readAsDataURL(audioBlob);
          }
        }, 200);
        
        // Start recording continuous stream
        audioRecorder.start();
        console.log('Started audio streaming');
        
      } catch (e) {
        console.error('Failed to start audio streaming:', e);
      }
    }
    
    function handleIncomingAudio(senderId, audioChunk) {
      // Don't play your own audio
      if (senderId === myUuid) {
        console.log('Skipping own audio');
        return;
      }
      
      try {
        // Get or create audio element for this sender
        let audio = audioElements.get(senderId);
        if (!audio) {
          audio = document.createElement('audio');
          audio.autoplay = true;
          audio.style.display = 'none';
          
          document.body.appendChild(audio);
          audioElements.set(senderId, audio);
          audioQueues.set(senderId, []);
          
          // Set up ended handler once
          audio.onended = () => {
            const q = audioQueues.get(senderId);
            if (q && q.length > 0) {
              const nextSegment = q.shift();
              audio.src = nextSegment;
              audio.play().catch(e => console.log('Queue playback failed:', e));
            }
          };
        }
        
        // Create audio URL from base64
        const audioUrl = `data:audio/webm;base64,${audioChunk.d || audioChunk.data}`;
        
        // Add to queue
        const queue = audioQueues.get(senderId);
        queue.push(audioUrl);
        
        // If audio is not playing and we have segments, start playing
        if ((audio.paused || audio.ended) && queue.length > 0) {
          const nextSegment = queue.shift();
          audio.src = nextSegment;
          audio.play().catch(e => {
            console.log('Audio playback failed:', e);
            // Don't try muted audio - that defeats the purpose
          });
        }
        
      } catch (e) {
        console.error('Failed to handle audio:', e);
      }
    }
    
    // ============= Firebase Peer Management =============
    
    async function registerPeer(peerId) {
      await database.ref(`peers/${peerId}`).set({
        timestamp: Date.now(),
        name: peerId
      });
      
      // Clean up on disconnect
      database.ref(`peers/${peerId}`).onDisconnect().remove();
      database.ref(`streams/${peerId}`).onDisconnect().remove();
    }
    
    async function getActivePeers() {
      const snapshot = await database.ref('peers').once('value');
      const peers = snapshot.val() || {};
      const activePeers = Object.keys(peers).filter(id => id !== myPeerId);
      return activePeers;
    }
    
    function listenForPeers() {
      database.ref('peers').on('child_added', (snapshot) => {
        const peerId = snapshot.key;
        if (peerId !== myPeerId) {
          console.log('New peer joined:', peerId);
          peers.set(peerId, {
            id: peerId,
            joinedAt: Date.now()
          });
        }
      });
      
      database.ref('peers').on('child_removed', (snapshot) => {
        const peerId = snapshot.key;
        console.log('Peer left:', peerId);
        peers.delete(peerId);
      });
    }
    
    let eventSequence = 0;
    
    // Send keyframe for initial sync and periodic refresh
    async function sendKeyframe(base64Data) {
      if (!isStreaming) return;
      
      const activePeers = await getActivePeers();
      if (activePeers.length === 0) return;
      
      const keyframe = {
        s: eventSequence++,
        t: Date.now(),
        k: base64Data, // Full keyframe
        isKeyframe: true
      };
      
      // Send keyframe on same path
      for (const peerId of activePeers) {
        database.ref(`streams/${peerId}/events_${myPeerId}`).set(keyframe)
          .catch(e => console.error(`Failed to send keyframe to ${peerId}:`, e));
      }
      
      console.log('Sent keyframe to', activePeers.length, 'peers');
    }
    
    // Send motion events through Firebase - continuous stream like original
    async function sendMotionEvents(events) {
      if (!isStreaming || !events || events.length === 0) return;
      
      const activePeers = await getActivePeers();
      if (activePeers.length === 0) return;
      
      // Create event package with sequence number
      const eventPackage = {
        s: eventSequence++,  // Sequence number to ensure updates
        t: Date.now(),
        f: frameCount,
        e: events // Events with pixel data
      };
      
      // Use set() on the same path - this creates a continuous stream
      // The receiver will get updates via 'value' listener
      for (const peerId of activePeers) {
        database.ref(`streams/${peerId}/events_${myPeerId}`).set(eventPackage)
          .catch(e => console.error(`Failed to send events to ${peerId}:`, e));
      }
    }
    
    // Listen for incoming motion events and audio
    function listenForStreams() {
      console.log(`Setting up stream listeners for ${myPeerId}`);
      
      const streamRef = database.ref(`streams/${myPeerId}`);
      
      streamRef.on('child_added', (snapshot) => {
        const key = snapshot.key;
        
        // Handle motion events
        if (key.startsWith('events_')) {
          const senderId = key.replace('events_', '');
          if (senderId === myPeerId) return;
          
          console.log(`New event stream detected from ${senderId}`);
          
          // Listen for continuous updates using 'value' 
          const eventsRef = database.ref(`streams/${myPeerId}/events_${senderId}`);
          let lastSequence = -1;
          
          eventsRef.on('value', (eventSnapshot) => {
            const eventPackage = eventSnapshot.val();
            if (eventPackage && eventPackage.e && eventPackage.s > lastSequence) {
              lastSequence = eventPackage.s; // Track sequence to avoid duplicates
              handleIncomingEvents(senderId, eventPackage);
            }
          });
        }
        
        // Handle audio streams
        else if (key.startsWith('audio_')) {
          const senderId = key.replace('audio_', '');
          if (senderId === myPeerId || senderId === myUuid) return;
          
          console.log(`New audio stream detected from ${senderId}`);
          
          // Listen for audio from this sender
          const audioRef = database.ref(`streams/${myPeerId}/audio_${senderId}`);
          audioRef.on('value', (audioSnapshot) => {
            const audioChunk = audioSnapshot.val();
            if (audioChunk && audioChunk.d) {
              handleIncomingAudio(senderId, audioChunk);
            }
          });
        }
      });
    }
    
    // Store reference frames for each remote peer
    const remoteFrames = new Map();
    const remoteCanvases = new Map();
    
    // Handle keyframe
    function handleKeyframe(senderId, keyframeData) {
      let remoteCanvas = document.getElementById(`remote-canvas-${senderId}`);
      if (!remoteCanvas) {
        // Create canvas if doesn't exist
        const demoGrid = document.querySelector('.demo-grid');
        const demoBox = document.createElement('div');
        demoBox.className = 'demo-box';
        demoBox.innerHTML = `
          <h3>üì° Remote: ${senderId}</h3>
          <canvas id="remote-canvas-${senderId}" width="${videoWidth}" height="${videoHeight}"></canvas>
          <div class="stats">
            <div class="stat-row">
              <span class="stat-label">Status:</span>
              <span class="stat-value" id="remote-events-${senderId}">Connected</span>
            </div>
            <div class="stat-row">
              <span class="stat-label">Frame:</span>
              <span class="stat-value" id="remote-frame-${senderId}">0</span>
            </div>
          </div>
        `;
        demoGrid.appendChild(demoBox);
        remoteCanvas = document.getElementById(`remote-canvas-${senderId}`);
      }
      
      const remoteCtx = remoteCanvas.getContext('2d');
      const img = new Image();
      img.onload = () => {
        remoteCtx.drawImage(img, 0, 0, videoWidth, videoHeight);
        // Save as reference frame
        const referenceFrame = remoteCtx.getImageData(0, 0, videoWidth, videoHeight);
        remoteFrames.set(senderId, referenceFrame);
      };
      img.src = `data:image/jpeg;base64,${keyframeData}`;
    }
    
    // Handle incoming motion events
    function handleIncomingEvents(senderId, eventPackage) {
      // Don't display your own video
      if (senderId === myPeerId || senderId === myUuid) {
        console.log('Skipping own events');
        return;
      }
      
      // Check if it's a keyframe
      if (eventPackage.isKeyframe && eventPackage.k) {
        console.log(`Received keyframe from ${senderId}`);
        handleKeyframe(senderId, eventPackage.k);
        return;
      }
      
      const events = eventPackage.e || [];
      console.log(`Received ${events.length} events from ${senderId}, frame ${eventPackage.f}`);
      
      // Get or create canvas for this sender
      let remoteCanvas = document.getElementById(`remote-canvas-${senderId}`);
      if (!remoteCanvas) {
        // Create a new demo box for this remote peer
        const demoGrid = document.querySelector('.demo-grid');
        const demoBox = document.createElement('div');
        demoBox.className = 'demo-box';
        demoBox.innerHTML = `
          <h3>üì° Remote: ${senderId}</h3>
          <canvas id="remote-canvas-${senderId}" width="${videoWidth}" height="${videoHeight}"></canvas>
          <div class="stats">
            <div class="stat-row">
              <span class="stat-label">Status:</span>
              <span class="stat-value" id="remote-events-${senderId}">Connecting...</span>
            </div>
            <div class="stat-row">
              <span class="stat-label">Frame:</span>
              <span class="stat-value" id="remote-frame-${senderId}">0</span>
            </div>
          </div>
        `;
        demoGrid.appendChild(demoBox);
        remoteCanvas = document.getElementById(`remote-canvas-${senderId}`);
      }
      
      const remoteCtx = remoteCanvas.getContext('2d');
      
      // Get or initialize reference frame
      let referenceFrame = remoteFrames.get(senderId);
      if (!referenceFrame) {
        // Start with black frame
        remoteCtx.fillStyle = 'black';
        remoteCtx.fillRect(0, 0, videoWidth, videoHeight);
        referenceFrame = remoteCtx.getImageData(0, 0, videoWidth, videoHeight);
        remoteFrames.set(senderId, referenceFrame);
      }
      
      // Draw reference frame
      remoteCtx.putImageData(referenceFrame, 0, 0);
      
      // Apply motion events with actual pixel data
      events.forEach(event => {
        if (event.d) {
          // Decode and draw the actual block image data
          const img = new Image();
          img.onload = () => {
            // If it's a motion vector, first copy from source position
            if (event.mv && (event.vx !== 0 || event.vy !== 0)) {
              // Copy block from where it came from
              const sourceX = event.x - event.vx;
              const sourceY = event.y - event.vy;
              if (sourceX >= 0 && sourceY >= 0 && sourceX < videoWidth && sourceY < videoHeight) {
                const movedBlock = remoteCtx.getImageData(sourceX, sourceY, event.s, event.s);
                remoteCtx.putImageData(movedBlock, event.x, event.y);
              }
            }
            
            // Draw the new block data
            remoteCtx.drawImage(img, event.x, event.y, event.s, event.s);
            
            // Update reference frame
            referenceFrame = remoteCtx.getImageData(0, 0, videoWidth, videoHeight);
            remoteFrames.set(senderId, referenceFrame);
          };
          img.src = `data:image/jpeg;base64,${event.d}`;
        }
      });
      
      // Update stats
      const statusEl = document.getElementById(`remote-events-${senderId}`);
      const frameEl = document.getElementById(`remote-frame-${senderId}`);
      if (statusEl) statusEl.textContent = `${events.length} events/frame`;
      if (frameEl) frameEl.textContent = eventPackage.f || 0;
    }
    
    // Initialize peer connection when camera starts
    async function initializePeerConnection() {
      // myPeerId is already set from character selection
      console.log('My peer ID:', myPeerId);
      
      await registerPeer(myPeerId);
      listenForPeers();
      listenForStreams();
      
      isStreaming = true;
      console.log('Peer connection initialized');
    }
    
    // Cleanup on page unload
    window.addEventListener('beforeunload', () => {
      if (myPeerId) {
        database.ref(`peers/${myPeerId}`).remove();
        database.ref(`streams/${myPeerId}`).remove();
      }
    });
  </script>
</body>
</html>